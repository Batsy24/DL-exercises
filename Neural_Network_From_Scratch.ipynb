{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "sQfIzWXMfAmj",
        "ylivjoRHnNP7",
        "iRVo5lRj0m02",
        "_tKFQuDgXHVm"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Neural Net from scratch** \n",
        "\n",
        "---\n",
        "\n",
        "Date Started: 22 Jan 2023 \\\n",
        "Date Finished: 1 Feb 2023 mostly cuz of procrastination\n",
        "\n"
      ],
      "metadata": {
        "id": "5WjvWO0KegZx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Imports***"
      ],
      "metadata": {
        "id": "sQfIzWXMfAmj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from keras.datasets import mnist\n",
        "from keras.utils import np_utils"
      ],
      "metadata": {
        "id": "yygUrY1AfJS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Code**"
      ],
      "metadata": {
        "id": "ylivjoRHnNP7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Dense:\n",
        "  def __init__(self, input_size, output_size):\n",
        "    self.i = input_size\n",
        "    self.j = output_size\n",
        "    self.weights = np.random.randn(self.i, self.j)/np.sqrt(self.i+self.j)  # xavier initialised, read paper for more. \n",
        "    self.bias = np.random.randn(1, self.j)/np.sqrt(self.i + self.j)\n",
        "\n",
        "  def forwardPropagation(self, input):\n",
        "    self.input = input \n",
        "    return np.dot(self.input, self.weights) + self.bias\n",
        "        \n",
        "  def backwardPropagation(self, output_gradient, learning_rate): # check derivations for all these in notes\n",
        "    input_gradient = np.dot(output_gradient, self.weights.T) # dE/dX\n",
        "    weight_gradient = np.dot(self.input.T, output_gradient) # dE/dW \n",
        "    bias_gradient = output_gradient # dE/dB\n",
        "\n",
        "    self.weights -= learning_rate * weight_gradient\n",
        "    self.bias -= learning_rate * bias_gradient # adjusting by negative of gradient\n",
        "\n",
        "    return input_gradient"
      ],
      "metadata": {
        "id": "NGk7jlRsoakw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Activation:\n",
        "  def __init__(self, activation, activation_derivative):\n",
        "    self.activation = activation \n",
        "    self.activation_derivative = activation_derivative\n",
        "\n",
        "  def forwardPropagation(self, input):\n",
        "    self.input = input\n",
        "    return self.activation(self.input) # f(x) where f is activation func and x is input matrix\n",
        "  \n",
        "\n",
        "  def backwardPropagation(self, output_gradient, learning_rate):\n",
        "    return output_gradient * self.activation_derivative(self.input) # dE/dY . f'(x)\n",
        "    "
      ],
      "metadata": {
        "id": "GVFAJG_gsMEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# kinda copy pasted this and FL bc ew but do learn it pls \n",
        "\n",
        "class SoftmaxLayer:\n",
        "    def __init__(self, input_size):\n",
        "        self.input_size = input_size\n",
        "    \n",
        "    def forwardPropagation(self, input):\n",
        "        self.input = input\n",
        "        tmp = np.exp(input)\n",
        "        self.output = tmp / np.sum(tmp)\n",
        "        return self.output\n",
        "    \n",
        "    def backwardPropagation(self, output_error, learning_rate):\n",
        "        input_error = np.zeros(output_error.shape)\n",
        "        out = np.tile(self.output.T, self.input_size)\n",
        "        return self.output * np.dot(output_error, np.identity(self.input_size) - out)"
      ],
      "metadata": {
        "id": "whiKAutEsREr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FlattenLayer:\n",
        "    def __init__(self, input_shape):\n",
        "        self.input_shape = input_shape\n",
        "\n",
        "    def forwardPropagation(self, input):\n",
        "        return np.reshape(input, (1, -1))\n",
        "    \n",
        "    def backwardPropagation(self, output_error, learning_rate):\n",
        "        return np.reshape(output_error, self.input_shape)"
      ],
      "metadata": {
        "id": "OStWSc8YmPe-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we only need the best activation, add tanh and sigmoid later tho\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(x, 0)\n",
        "\n",
        "def relu_prime(x):\n",
        "    return np.array(x >= 0).astype('int')\n",
        "\n",
        "\n",
        "\n",
        "# impliment binary cross entropy and stuff later, mse for now. also add accuracy metrics bc why not\n",
        "\n",
        "def mse(y_true, y_pred):\n",
        "    return np.mean(np.power(y_true - y_pred, 2))\n",
        "\n",
        "def mse_prime(y_true, y_pred):\n",
        "    return 2 * (y_pred - y_true) / y_pred.size"
      ],
      "metadata": {
        "id": "zKxeK1Ou4fIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Network:\n",
        "    def __init__(self, loss, loss_prime):\n",
        "        self.layers = []\n",
        "        self.loss = loss\n",
        "        self.loss_prime = loss_prime\n",
        "\n",
        "    def add(self, layer):\n",
        "        self.layers.append(layer)\n",
        "\n",
        "\n",
        "    def predict(self, input_data):\n",
        "        samples = len(input_data)\n",
        "        result = []\n",
        "\n",
        "        for i in range(samples):\n",
        "            # dis is just forward propagation but every time u switch layers the prev layer output becomes the func input\n",
        "            output = input_data[i]\n",
        "            for layer in self.layers:\n",
        "                output = layer.forwardPropagation(output)\n",
        "            result.append(output)\n",
        "\n",
        "        return result\n",
        "\n",
        "\n",
        "    def fit(self, x_train, y_train, epochs, learning_rate):\n",
        "\n",
        "        samples = len(x_train)\n",
        "\n",
        "        for i in range(epochs):\n",
        "            loss = 0\n",
        "            for j in range(samples):\n",
        "                # forward propagation\n",
        "                output = x_train[j]\n",
        "                for layer in self.layers:\n",
        "                    output = layer.forwardPropagation(output)\n",
        "\n",
        "                # technically u can remove but TF aesthetics amirite\n",
        "                loss += self.loss(y_train[j], output)\n",
        "\n",
        "                # backward propagation\n",
        "                error = self.loss_prime(y_train[j], output) # dE/dY\n",
        "                for layer in reversed(self.layers):\n",
        "                    error = layer.backwardPropagation(error, learning_rate)\n",
        "\n",
        "            # calculate average error on all samples\n",
        "            loss /= samples\n",
        "            print('epoch %d/%d   error=%f' % (i+1, epochs, loss)) # copy pasted print statement lmao"
      ],
      "metadata": {
        "id": "1XGoOmNiOBw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Test #1**\n",
        "\n",
        "### XOR\n",
        "\n",
        "Exclusive-Or logic gate:\n",
        "\n",
        "| A \t| B \t| Z \t|\n",
        "|---\t|---\t|---\t|\n",
        "| 0 \t| 0 \t| 0 \t|\n",
        "| 0 \t| 1 \t| 1 \t|\n",
        "| 1 \t| 0 \t| 1 \t|\n",
        "| 1 \t| 1 \t| 0 \t|"
      ],
      "metadata": {
        "id": "iRVo5lRj0m02"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = np.array([[[0,0]], [[0,1]], [[1,0]], [[1,1]]])\n",
        "y_train = np.array([[[0]], [[1]], [[1]], [[0]]])\n",
        "print(f' x_train shape: {x_train.shape} \\n y_train shape: {y_train.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIPVY2lK06Yi",
        "outputId": "8625b7ce-6dd8-4333-dc61-a77a37fa73f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " x_train shape: (4, 1, 2) \n",
            " y_train shape: (4, 1, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xorNet = Network(mse, mse_prime)\n",
        "\n",
        "architecture = [\n",
        "    Dense(2,3),\n",
        "    Activation(relu, relu_prime),\n",
        "    Dense(3,1),\n",
        "    Activation(relu, relu_prime)\n",
        "]\n",
        "\n",
        "for layer in architecture:\n",
        "  xorNet.add(layer)"
      ],
      "metadata": {
        "id": "UvLavG1D3PgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xorNet.fit(x_train, y_train, epochs=300, learning_rate=0.1)"
      ],
      "metadata": {
        "id": "JzyiBDRw4D6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = xorNet.predict(x_train)\n",
        "\n",
        "n = 0\n",
        "for dat in x_train:\n",
        "  for sub_dat in dat:\n",
        "    print(f'Input: {sub_dat}')\n",
        "    print(f'Prediction: {round(test[n][0][0])}')\n",
        "    print(f'True value: {y_train[n][0][0]} \\n')\n",
        "    n+=1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfxFsTRd4NWe",
        "outputId": "3210fb5a-00fe-4b8e-badf-2402a44bcd64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: [0 0]\n",
            "Prediction: 0\n",
            "True value: 0 \n",
            "\n",
            "Input: [0 1]\n",
            "Prediction: 0\n",
            "True value: 1 \n",
            "\n",
            "Input: [1 0]\n",
            "Prediction: 0\n",
            "True value: 1 \n",
            "\n",
            "Input: [1 1]\n",
            "Prediction: 0\n",
            "True value: 0 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Test #2**\n",
        "### MNIST\n",
        "\n",
        "Handwritten digit images."
      ],
      "metadata": {
        "id": "_tKFQuDgXHVm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "y_train = np_utils.to_categorical(y_train) # turns it from (60k,) to (60k, 10), basically probability matrix\n",
        "x_train /= 255\n",
        "\n",
        "x_test = x_test.astype('float32')\n",
        "y_test = np_utils.to_categorical(y_test)\n",
        "x_test /= 255\n",
        "\n",
        "# notes:\n",
        "# training data shape: (60k, 28, 28), (60k, 10)\n",
        "# testing data shape: (10k, 28, 28), (10k, 10)\n",
        "# standard scaler has weird implications here, gives NaN values in the error...look into that. I feel like not standardising Y should fix it.\n",
        "# normalising with /255 doesnt rlly do much excpect make the net quicker, \n",
        "# standard scaler SHOULD make it more accurate too"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-wG6gTJXYwy",
        "outputId": "b386560d-c222-4fb3-c823-f5fbc49ac631"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m82NTWflkTfI",
        "outputId": "3b5d5f48-3b1e-4388-a956-ee1bd0378265"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mnistNet = Network(mse, mse_prime)\n",
        "\n",
        "architecture = [\n",
        "    FlattenLayer(input_shape=(28, 28)),\n",
        "    Dense(28*28, 64),\n",
        "    Activation(relu, relu_prime),\n",
        "    Dense(64, 32),\n",
        "    Activation(relu, relu_prime),\n",
        "    Dense(32, 10),\n",
        "    SoftmaxLayer(10)\n",
        "]\n",
        "\n",
        "for layer in architecture:\n",
        "  mnistNet.add(layer)\n",
        "\n",
        "epochs = 50\n",
        "alpha = 0.2\n"
      ],
      "metadata": {
        "id": "EI2UrLMpb-Vu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mnistNet.fit(x_train[0:10000], y_train[0:10000], epochs=epochs, learning_rate=alpha)"
      ],
      "metadata": {
        "id": "j-5S278PeLHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predict = mnistNet.predict(x_test)\n",
        "\n",
        "# index = 21\n",
        "\n",
        "# for value in predict[index]:\n",
        "#   print(f'prediction: {np.argmax(value)}')\n",
        "#   print(f'true value: {np.argmax(y_test[index])}')"
      ],
      "metadata": {
        "id": "7yBJww74qRgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samples = 10\n",
        "predictions = mnistNet.predict(x_test[:samples])\n",
        "n = 0\n",
        "\n",
        "for test, true in zip(x_test[:samples], y_test[:samples]):\n",
        "    image = np.reshape(test, (28, 28))\n",
        "    plt.imshow(image, cmap='binary')\n",
        "    print('Input Image: ')\n",
        "    plt.show()\n",
        "\n",
        "    pred_value = np.argmax(predictions[n])\n",
        "    true_value = np.argmax(true)\n",
        "\n",
        "    print(f' Predicted Value: {pred_value} \\n True Value: {true_value} \\n Confidence: {predictions[n][0][pred_value]}')\n",
        "\n",
        "    n += 1"
      ],
      "metadata": {
        "id": "moaBA-CueWbt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}